import requests
import csv
import os
import re
from bs4 import BeautifulSoup

target_date = "2023-02-20"

url = "https://www.exploit-db.com/"
headers = {
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "User-Agent": "Mozilla/5.0"
}

start = 0
batch_size = 1000
params = {
    "start": start,
    "length": batch_size
}

collected = []

def extract_info(entry):
    return {
        "id": entry.get("id"),
        "title": entry.get("description", ["", ""])[1],
        "content": entry.get("description", ["", ""])[0],
        "date": entry.get("date_published"),
        "cve": f"CVE-{entry.get('code', [{}])[0].get('code', 'N/A')}" if entry.get("code") else "N/A",
        "platform": entry.get("platform", {}).get("platform", "N/A"),
        "type": entry.get("type", {}).get("display", "N/A"),
        "author": entry.get("author", {}).get("name", "N/A")
    }

def clean_text(html_content):
    soup = BeautifulSoup(html_content, "html.parser")

    for tag in soup(['script', 'style', 'iframe']):
        tag.decompose()

    text = ' '.join(soup.stripped_strings)

    text = re.sub(r'\[image:\s*[^]]+\]', '', text)

    lines = [
        line for line in text.splitlines()
        if re.search(r'[a-zA-Z0-9]', line) and not line.strip().startswith('//')
    ]

    return ' '.join(' '.join(lines).split())

def fetch_exploit_text(exploit_id):
    html_url = f"https://www.exploit-db.com/exploits/{exploit_id}"
    html_resp = requests.get(html_url, headers=headers)
    soup = BeautifulSoup(html_resp.text, "html.parser")
    code_block = soup.find("pre")
    if not code_block:
        return ""

    full_text = code_block.get_text()
    truncated_text = re.split(r'\bPoC\b', full_text, flags=re.IGNORECASE)[0]
    return clean_text(truncated_text)

print(f"Fetching exploits {start} to {start+batch_size-1}...")
response = requests.get(url, headers=headers, params=params)
data = response.json()
total_exploits = data.get("recordsTotal", 0)

for entry in data.get("data", []):
    if entry.get("date_published") == target_date:
        collected.append(extract_info(entry))

for start in range(batch_size, total_exploits, batch_size):
    params["start"] = start
    print(f"Fetching exploits {start} to {start+batch_size-1}...")
    response = requests.get(url, headers=headers, params=params)
    data = response.json()
    for entry in data.get("data", []):
        if entry.get("date_published") == target_date:
            collected.append(extract_info(entry))

print(f"\n✅ Collected {len(collected)} exploits from {target_date}.\n")

output_file = "exploitdb_cve_cleaned.csv"
file_exists = os.path.exists(output_file)

with open(output_file, "a", newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=["cve", "timestamp", "source", "text"])

    if not file_exists:
        writer.writeheader()
    
    for e in collected:
        cve = e["cve"].strip().upper()
        if not cve.startswith("CVE-") or not re.match(r'CVE-\d{4}-\d+', cve):
            continue

        cve_list = [c.strip() for c in cve.split(",")]

        for single_cve in cve_list:
            cve_counts = {single_cve: 1}
            writer.writerow({
                "cve": single_cve,
                "timestamp": e["date"],
                "source": "exploit db",
                "text": fetch_exploit_text(e["id"])
            })

print("✅ CSV file updated with separate entries for each CVE.")