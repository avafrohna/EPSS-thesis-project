import requests
import json
import csv
import os
import re
from bs4 import BeautifulSoup

target_date = "2025-04-16"

url = "https://www.exploit-db.com/"
headers = {
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "User-Agent": "Mozilla/5.0"
}

start = 0
batch_size = 1000
params = {
    "start": start,
    "length": batch_size
}

collected = []

def extract_info(entry):
    return {
        "id": entry.get("id"),
        "title": entry.get("description", ["", ""])[1],
        "content": entry.get("description", ["", ""])[0],
        "date": entry.get("date_published"),
        "cve": entry.get("code", [{}])[0].get("code", "N/A") if entry.get("code") else "N/A",
        "platform": entry.get("platform", {}).get("platform", "N/A"),
        "type": entry.get("type", {}).get("display", "N/A"),
        "author": entry.get("author", {}).get("name", "N/A")
    }

def clean_text(html_content):
    soup = BeautifulSoup(html_content, "html.parser")

    # Remove unwanted tags
    for tag in soup(['script', 'style', 'iframe']):
        tag.decompose()

    # Extract and join all text nodes
    text = ' '.join(soup.stripped_strings)

    # Remove markdown image tags like [image: ...]
    text = re.sub(r'\[image:\s*[^]]+\]', '', text)

    # Remove lines starting with // (likely code comments)
    lines = [line for line in text.splitlines() if not line.strip().startswith('//')]

    # Remove known unwanted phrases
    cleaned = " ".join(lines)
    cleaned = cleaned.replace("Make me a prince!", "")

    # Collapse all whitespace to single spaces
    cleaned = " ".join(cleaned.split())
    return cleaned

def fetch_exploit_text(exploit_id):
    html_url = f"https://www.exploit-db.com/exploits/{exploit_id}"
    html_resp = requests.get(html_url, headers=headers)
    soup = BeautifulSoup(html_resp.text, "html.parser")
    code_block = soup.find("pre")
    if not code_block:
        return ""

    full_text = code_block.get_text()
    # Remove everything after 'PoC' (case-insensitive)
    truncated_text = re.split(r'\bPoC\b', full_text, flags=re.IGNORECASE)[0]
    return clean_text(truncated_text)

print(f"Fetching exploits {start} to {start+batch_size-1}...")
response = requests.get(url, headers=headers, params=params)
data = response.json()
total_exploits = data.get("recordsTotal", 0)

for entry in data.get("data", []):
    if entry.get("date_published") == target_date:
        collected.append(extract_info(entry))

for start in range(batch_size, total_exploits, batch_size):
    params["start"] = start
    print(f"Fetching exploits {start} to {start+batch_size-1}...")
    response = requests.get(url, headers=headers, params=params)
    data = response.json()
    for entry in data.get("data", []):
        if entry.get("date_published") == target_date:
            collected.append(extract_info(entry))

print(f"\n✅ Collected {len(collected)} exploits from {target_date}.\n")

formatted = []
for e in collected:
    cve = e["cve"]
    cve_list = [cve] if cve != "N/A" else []
    cve_counts = {cve: 1} if cve != "N/A" else {}

    if cve_list:
        formatted.append({
            "cves": cve_list,
            "cve_counts": cve_counts,
            "title": e["title"],
            "date": e["date"],
            "link": f"https://www.exploit-db.com/exploits/{e['id']}",
            "text": fetch_exploit_text(e["id"])
        })

file_exists = os.path.exists("exploitdb_cve.csv")

with open("exploitdb_cve.csv", "a", newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=["cves", "cve_counts", "date", "title", "link", "text"])
    if not file_exists:
        writer.writeheader()
    for item in formatted:
        writer.writerow({
            "cves": ";".join(item["cves"]),
            "cve_counts": json.dumps(item["cve_counts"]),
            "date": item["date"],
            "title": item["title"],
            "link": item["link"],
            "text": item["text"]
        })

print("✅ CSV file saved as exploitdb_cve.csv.")